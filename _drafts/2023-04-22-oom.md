---
title: 内存背压服务
---

# 背景

数据处理服务对单挑消费数据做批量处理, 一来提高数据去重, 减少计算; 二来利用批量IO的方式提高读写速度.
批量开的越大, 去重率越高, 当然效果越好, 缺点在于下游延迟增加, 以及单批处理崩溃导致丢失的数据范围较大.

```
buffer = {}
for key, payload in consumer.consume():
    # filter logic here
    buffer[key] = payload
    if len(buffer) >= limit:  # or timeout ...
        process_batch(buffer.values())
        buffer = {}
```

PY里面最简单的拿字典做缓存对象, 当然也可以用基于文件或者外部对象做批数据缓存, 但是对于业务代码调用限制要求较多, 额外的序列化及外部IO会影响计算速度, 故不讨论.

内存是有限的, 我们需要尽可能的提高内存资源利用率 (从而提高系统吞吐), 且不触发OOM. 尤其在docker/k8s的运行环境下, 资源隔离的更加细粒度.

此外单PY程序跑起来的常驻开销也很大, 而实际数据处理导致的内存开销波动较少, 通过合并数据处理服务, 可显著减少整体内存占用.
可以通过设置更高的内存上限, 提高单消费可以做的批大小上限, 从而提高资源利用率.

然而这就对内存控制有更加精细的要求和挑战, 需要感知到内存快满时, 触发(部分)数据清理逻辑. 否则一旦触发OOM, 未处理的数据直接丢失得不偿失.

PY对象内存占用很难精确估计, 当然如果输入全是数值类型的话还相对稳定, 如果处理的是其他字符串等结构, 基于批量大小的推测不够安全.

# 理解内存


```
> free -h
               total        used        free      shared  buff/cache   available
Mem:            31Gi       729Mi        11Gi       1.0Mi        19Gi        30Gi
Swap:          8.0Gi        83Mi       7.9Gi

> ps uxf

USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   1804  1188 ?        Sl   16:49   0:00 /init
root         7  0.0  0.0   1812    88 ?        Ss   16:49   0:00 /init
root         8  0.0  0.0   1812    96 ?        S    16:49   0:00  \_ /init
root         9  0.0  0.1   6328  5332 pts/0    Ss   16:49   0:00      \_ -bash
root       447  0.0  0.0   7476  3140 pts/0    R+   19:07   0:00          \_ ps auxf

> top

MiB Mem :   4006.7 total,   1468.2 free,   1462.9 used,   1075.6 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   2325.1 avail Mem

PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                                                                                                                                             
  8 root      20   0 1621612 646728  94420 S  62.7  33.1   1:19.01 python manage.py
                                                                                                                              
```

- buff/cache
  - 操作系统管理的, 可以认为全部收回利用的, 因此计算可用 (available / avail Mem) 内存的时候不纳入考虑
  - buffer = kernel buffer
  - cache = file page cache
- VIRT = VSZ * PAGE_SIZE
  - virtual memory
  - 单进程视角认为它分配的内存大小
  - ref process_resident_memory_bytes
  - cat /proc/$pid/stat
  - ref virtual_memory_bytes / process_virtual_memory_bytes
  - 可以超过物理内存+SWAP大小, 因为申请的内存实际上没使用就没事儿, 实际请求读写时才触发分配
- RES = RSS * PAGE_SIZE
  - RSS (Resident Set Size) 实际在内存的页数
  - anything occupying physical memory
- SHR
  - RES的一部分, 不包含程序主动申请的内存, 以及栈空间, 可以认为是加载程序/库+IO

behind the scene

- 全局: cat /proc/meminfo
- 进程层面: man proc
  - /proc/$pid/stat

VMM (virtual memory mapping) / 管理进程视角内存到实际物理地址, 因此内存需要分页, 以便于检索

/proc/$pid/maps
/proc/$pid/pagemap

16GB内存 / 4KB = vmm 空间诉求 = ???

内存相关的中断
- minor page fault: 实际触发内存页分配
- major page fault: 同步磁盘数据块到内存页, 格外慢

# SWAP

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/getting-started-with-swap_managing-storage-devices

- 物理内存空间不足时, 选择部分内存页缓存到磁盘, 相关内存页
- SWAP越大越好么 ???
- SWAP比率过高时, 实际用在有效计算上的周期少, 这就看是包速度, 还是尽量省内存了

# OOM过程

虚拟内存页 VM / SWAP

# cgroup / Docker / k8s

- /sys/fs/cgroup/memory/memory.limit_in_bytes 内存限制
- /sys/fs/cgroup/memory/memory.usage_in_bytes 当前使用量
- /sys/fs/cgroup/memory/memory.max_usage_in_bytes 最大使用量
- /sys/fs/cgroup/cpu/cpu.cfs_quota_us CPU限制
- /sys/fs/cgroup/cpu/cpuacct.usage CPU使用

https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt

https://jvns.ca/blog/2017/02/17/mystery-swap/

cgroup swap limit ???

WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.

k8s内存相关监控指标

- container_memory_working_set_bytes
- container_memory_usage_bytes = container_memory_rss + container_memory_cache + container_memory_swap + kernel memory

https://faun.pub/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d

# 内存分配

- vmm 管理实际内存页, 管理逻辑内存地址和实际内存地址映射
- brk/sbrk: 系统调用, 连续内存空间视角
- malloc / free: 标准库接口, 非系统调用, 内存片段管理问题
  - [tcmalloc](https://github.com/google/tcmalloc)
  - [mimalloc](https://github.com/microsoft/mimalloc)
  - jemalloc
- 语言层面的内存池/待GC对象/...

# MMAP

共享内存机制

- 是否文件映射 anonymous / file backended
- 是否共享 private / shared

对于要常态加载静态数据的服务, 可以通过MMAP方式节约内存空间, 如模型文件 / numpy向量等, 当然缺点就是慢, 这就在执行效率/速度, 和内存占用上找折衷了.

https://en.wikipedia.org/wiki/Mmap

只读 / 读写 / ... 模式


用于跨进程数据共享, 同时也可以部分实现超过内存大小的计算.

场景: numpy特征向量, 相似度计算, 可以mmap出来后遍历. 当然缺点就是慢.

# Python GC

https://docs.python.org/3/library/gc.html

# Python内存友好代码指南

优选数据结构
- tuple over list
- list over dict
- use namedtuple or dataclass
- Cursor over DictCursor

尽可能使用迭代器机制

不可控环节做防守
- 涉及数据库读相关容易炸, 不过要避免使用SSCursor, 因为压力实际上还是在数据库, 应在查询语句上做文章 (如加limit, 加分页轮询等) 确保每个结果集可控
- 外部请求, requests读响应内容时, 很容易炸内存, 要么自己实现消费body逻辑, 限制读取大小, 要么转到临时文件磁盘扛着

避免复杂的逻辑结构, 避免死亡互相引用对象

避免嵌套的字典

数据结构 VS 计算复杂度

集合包含 VS 排序序列包含

# Reference

https://help.aliyun.com/document_detail/413870.html

https://docs.docker.com/config/containers/runmetrics/

https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66
