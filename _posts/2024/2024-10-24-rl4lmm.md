---
title: 深度强化学习
published: false
---

AI / 人工智能: 目的角度阐释, 机器实现类似人的能力

ML / 机器学习: 手段角度理解, 专家系统等基于规则的手段不算ML, 但也可以实现AI

model + training / 结构假设 + 训练手法

DL / 深度学习: ML的细分领域, 模型结构上

SL / UL / RL: 训练手段上来划分
- SL 刷有标准答案的卷子
- UL 书读百遍其意自现
- RL 不断碰壁不断尝试, 从自身的经验中学习

规则经验的传授 / heuristics -> 创造足够的环境和机制以自学成才

为什么要重视RL?

无监督的学习, 自学习需要不停投喂数据, 高质量的数据已经不够了. 需本身不断产生数据试错中来学习. 

RL+DL如何成双结对的? DL用于做函数近似

抛开计算能力, 一切都有最优选择. 奇异博士利用时间宝石看到未来不通的可能性, 于是穿越回来告诉当下的一步如何走

# 棋类游戏

tree search / 每步尝试多少中可能 / 多看多少步

- 剪枝
  - 深度裁剪, 不要往前看那么多步
  - 宽度裁剪, 每步指评估几种选项

最重要的是局势评价的策略
- 经验规则总结, 如数子等, 但永远是短视的, 不足的, 对于象棋等, 数数车马炮基本上就了然局势了, 但是对于围棋很难
- 随机乱下模拟结果, 从而以一定置信度判断当下局势是否有优势, 优势几何
- value function

MCTS: 在检索树上不断的模拟结果, 每个节点都用当前模拟的胜率, 优先的模拟计算资源下, 应该再多模拟哪些局势.
回到了最初的摇臂赌博机的问题以及EE策略的讨论.

minimax的局限性: ???

模型需要: 1. 正确地评估形式; 2. 从而选择当下最优的动作.


# 语言模型视作一场RL问题

action: 下一个字的选择

难点: 如何评价输出结束后的质量, 棋类游戏有明确的输赢, 生成的文章评价很难客观.

能够客观评价的, 是否存在事实谬误, 逻辑推导数学计算等是否正确.

# RLHF: reinforcement learning with human feedback

生成的文本, 人类评估标注质量, 作为奖励给模型.

缺点: 样本量 ???
