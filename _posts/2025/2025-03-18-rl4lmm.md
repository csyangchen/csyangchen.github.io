---
title: 深度强化学习
---

AI / 人工智能: 目的角度阐释, 机器实现类似人的能力

ML / 机器学习: 手段角度理解, 专家系统等基于规则的手段不算ML, 但也可以实现AI

机器学习两个要素: model + training / 结构假设 + 训练手法

结构假设
- 是否概率模型, 是否参数化模型, ... 
- 传统结构: 决策树, KNN, 贝叶斯, HMM, CRF等, 线性分类器 (以及核方法转换成线性结构), 很强的结构
- DL / 深度学习: 简单结构堆叠, 函数拟合
  - RNN/CNN/Attention... 更弱化的结构假设
  - XXNet... 更容易训练的动的结构
- 结构假设越弱, 学习参数越多, 训练越难

训练手法
- SL / 监督学习, 模仿学习, 刷有标准答案的卷子
- UL / 无监督学习, 书读百遍其意自现
- RL / 强化学习, 不断实践, 从经验中学习

为什么要重视RL?
- SL, UL需要不停投喂数据, 高质量的数据已经不够了, 需本身不断产生数据, 从试错中来学习
- RL是更为实际的问题表述形式
- 对于既有问题的规则经验的传授 (heuristics) 是有限的, 不能说差老师教不出好学生, 但是可以说很难教出远超老师的学生. 要走的更远, 探索未至之境

UL vs RL: RL更主动的定向的生成数据做UL, RL更目标导向, 而非结构导向

# RL回顾

- 状态空间, 行动空间, 奖励, MDP问题形式
- 行动价值函数与状态价值函数, Bellman方程
- 基于评价的算法 / value-learning
  - 评估每种状态的价值, 策略自然浮现, 不过依赖状态空间的可枚举性及低维度, 以及会导致策略的确定性
  - Q-learning, SARSA, ...
- 纯策略算法, policy gradient
  - 不做价值函数近似, 直接策略空间找最优解
  - RINFORCE: policy gradient with MC
- Actor-Critics methods: Actor: 演员, 策略函数; Critics: 评论家, 价值函数.
  - 感觉不能视作纯策略算法, 是策略和评价共同逼近迭代
  - A2C: Advantage Actor Critic
  - [A3C](https://arxiv.org/pdf/1602.01783): Asynchronous Advantage Actor-Critic Algorithm

# DRL / 深度强化学习

RL+DL如何勾搭上的? RL问题形式, DL具体干活 (做函数近似)

评估函数 / 回归问题: 预测当前胜率

策略函数:
- 易: 规则拟合能力, 输出合规的落子位置; 根据游戏画面, 预测下一帧, 等等
- 难: 每个落子的选择概率, 下一步行动选择

> DEMO: DNN拟合黑白棋容许落子位置

DRL推动者: Deepmind by Google 

# DQN

[DQN](https://deepmind.google/discover/blog/deep-reinforcement-learning/): deep Q-network, Q-learning with deep convolutional ANN.

DRL开山?

回顾RL里面, Q函数即价值函数, Q学习即从拟合价值函数角度来做.

Atari上一系列电子游戏 (类比小霸王学习机).
直接用视觉信号作为输入, 从而省掉了需要针对各种具体游戏的特征构建.
联合最近几帧当作输入, 从而更好的模拟出状态/MC特性.
对比视觉的状态空间, 动作空间很小, 就手柄的几个方向和按钮.
也形成了一系列RL算法评价的数据集.

算法策略的贡献: experience replay 提升训练的稳定性.

游戏是天然的RL训练场, 模拟成本低, 环境可控, [gymnasium](https://gymnasium.farama.org/index.html)项目.

# 打游戏时候在干什么?

- 规则学习, 如看剧情, 解锁新内容等, 知道什么地方有什么东西, 可以干什么事情等等
  - 棋类规则非常简单, 相比而言电子游戏规则则非常复杂, 阅读规则本身, 很多时候是玩游戏的乐趣
- 目标达成策略研究. 什么是攻略: 就是给定目标的最优行动策略
- 敏捷性练习. 如赛车, 射击, 及时战略等对于反应能力(即计算速度)要求比较高, 形成不加思考的条件反射机制, 孰能生巧, 等于在训练针对一款游戏的轻量模型, 以及更进一步打磨策略模型执行细节
  - 回合制等则没有太大时间压力, 每一回合需要的计算量比较大, 不太需要敏捷性练习, 更侧重于策略学习角度

# 棋类游戏

下棋: 确定性的, 有完备信息的, 两人参与的, 轮流行动的, 零和游戏

Minimax策略: 遍历所有的可能性及对应的对手选项, 找出最损人利己的步骤.

检索树 / 博弈树 / tree search: 边为每种选择, 根节点为当前状态, 节点为状态, 叶子节点(如有)为每种终局状态 

实施的缺陷: 计算局限, 看不到每种可能的终局

抛开计算能力, 一切都能看清, 有最优选择. 奇异博士利用时间宝石看到未来所有的可能性, 于是穿越回来告诉当下的一步如何走.

剪枝以确保计算可行性
- 深度裁剪, 往前看几步就好, 不要下到终局
- 宽度裁剪, 每步只挑几种选项做评估

衍生问题:
- 深度裁剪问题: 还没下完, 如何评估局势?
- 宽度裁剪问题: 每步这么多选项, 选哪几个尝试?

问题的难点在于快速的评估局势, 或者说胜率.

局势评估方法
1. 基于规则, 如数子. 但永远是短视, 不足的, 对于象棋等, 数数车马炮, 局势基本能明了了, 但是对于围棋来说很难, 数子不代表优势, 空间依赖复杂性高
2. 基于MC, 跑模拟, rollout. 随机乱下模拟结果, 从而以一定置信度判断胜率

MCTS: 在检索树上不断的模拟结果,, 有限的模拟计算资源下, 应该再多模拟哪些选项.
有限的计算资源如何分配以最大化信息, 探索新的选择, 还是再充分评估现有选项? EE困境

跑模拟时用什么策略也是个问题, 完全随机的策略收敛太慢, 一般用一个弱很多的策略, 视作模型集成, 所谓"三个臭皮匠赛一个诸葛亮".

minimax的局限性: 假设对手是理性的, 同策略的. 实际经验中, 经常会有乱拳打死老师傅, 不按套路出牌, "新手光环"的情况.
如何利用Minimax的局限性: 跳出对手对自己的假设.

TODO 在何种问题假设下, minimax策略能够保证最优?

# AlphaGo (AG)

围棋为什么难? 维度更大, 局势更难评估, 象棋等胜负更依赖单子能力, 相对较为容易.

独立训练三个深度模型:
- 轻策略模型: 用于跑模拟, 棋谱监督学习后就固定不再更新, 从而能有限时间内足够快的跑模拟结果出来
- 重策略模型: 考虑当下每个落子的概率, 可以接受更高的计算量要求
- 价值模型: 胜率评估, 视作跑模拟的结果近似

实际胜率评估 = r * v(s) + (1-r) * G, 即模拟结果和价值模型结果的组合, r参数代表更信任价值模型还是模拟结果.

Quiz: 策略模型/价值模型是分别用于裁剪宽度还是高度?

用棋谱做监督学习冷启动, 然后self-play生成数据, 训练更新重策略模型和价值模型; 轻策略模型不重要, 只要比乱下模拟收敛快就好.

MCTS+CNN网络, 以及工程上的胜利

[AlphaGo Zero (AGZ)](https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/), 更清爽的AG版本
- 不看棋谱, 纯self-play
- 一个更大的CNN网络, 合并了策略和价值模型, 直接输出两个头 (Quiz: 哪两个?)
- 不跑模拟, 省去了轻策略模型
- 更简单的棋局编码方式

注意AG的重策略网络和价值网络是单独的两个模型训练, 但是实际评价策略又是基于MCTS的. AGZ消除了这种异构性.

AGZ更为强大, 训练所需算力更少.

[AlphaZero (AZ)](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/):
不局限于围棋, 三种棋同时学, 为了验证方法不是特化围棋有效?

[MuZero](https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/):
规则都不知道, 取得了比DQN更好的效果

[AlphaFold](https://deepmind.google/technologies/alphafold/), 应用于更实际的科研问题

适合解决的问题场景: 每种路线都是可以去验证的, 但是路线太多, 全部验证成本很高.

# LM视作RL任务

LLM训练阶段
1. (UL) 基座模型 / 预训练模型 / 无监督NTP训练的结果
2. (SL) 对话模型 / SFT: Instruct-前缀, 输出对齐人类偏好 (如符合事实, 有用, 无害, 更善于对话, 有礼貌, 等等)
3. (RL) 推理模型: 进一步强化学习的产物

NTP训练任务的不合理性:
预测下一个字, 实际上非常多的字都可以作为下一个字.
对于下棋的监督训练, 按照棋谱预测下一个落子. 用菜鸟的棋谱训练, 必定越练越差.
NTP是一个收敛非常慢, 严重依赖训练数据质量的, 模仿学习过程.

对于推理任务, 怎么说不重要, 重要的是最终的答案对不对, 以及每一步推理环节是否自洽满足基本逻辑规则 (当然错误的推理过程, 错上加错, 得到正确的结果的可能性很小).
退一步, 即便没有得到最终正确答案, 推理过程也有过程分, 应该给予奖励. NTP形式不适合做推理类任务训练.

LM的RL表达形式:
- 行动空间: 词表大小
- 策略: 下一个字的选择
- 状态空间: 整段输出文本的可能性
- 奖励: 最终输出内容的质量评价

# RLHF / reinforcement learning with human feedback

模型输出多个结果, 人类标注结果偏好性, 用来训练一个结果评分器, 即奖励模型. 然后用奖励模型作为损失训练生成模型.

为什么先做评价模型再做生成模型的路子可以走得通? discriminator - generator gap / 做评论家很容易, 但是做创作者很难. 这只是个假设, 不一定成立.

注意, 这里奖励模型不同于评价模型. 奖励模型, 生成结束后才有; 评价模型, 生成过程中对于最终奖励的估测.

RLHF被认为不是RL的原因, 这里的奖励模型是偏主观的, 且有被过拟合的风险.

# 可验证问题 VS 不可验证问题

可验证问题: 
- 数学题, 逻辑推理题
- 给定一个问题/需求, 写个算法/程序, 是可验证对错的, 即便不能理论证明正确性, 也可以通过不断充分的测试用例来覆盖确保正确性
- 下棋, 打牌, 游戏
- 炒股
- 阅读理解里面的事实问答类问题, 视作推理题
- ...
- 客观题, 对就是对, 错就是错, 得多少分就是多少分

不可验证问题:
- 讲个笑话, 说些话哄女朋友开心
- 画个美女
- 是否擦边视频, 是否低质素材
- 提取文案卖点
- ...
- 主观题, 不存在对错, 主要看受众是否喜欢, 不同人评价难形成一致性

对于可验证问题, 不需要做奖励模型, 只要结果对, 过程怎样都可以, 甚至这个自创的推理路线, 通过语言表达出来之后, 对于人类也有很大的启发意义.

主观问题的量化: 规则总结, 和参考答案的距离, 等等, 但始终不能替代人类主观评价感受.

# 奖励模型的利弊

好处: 尽可能少的人力参与, 才能做大.

奖励模型代替人类评估的坏处: RL非常善于作弊, 生成模型很容易"玩弄"奖励模型, 尤其学习模型参数量绝对碾压评价模型时, 在不断的学习中总能找到并利用评价模型的"漏洞", 导致结果离人类的喜好越来越远.

因此不能使劲通过奖励模型来训练生成模型, 需适可而止, 否则会越训练越退化.

对于可验证问题, 或者说推理性问题, 使劲训练有奇效. 推理任务, 结论正确最重要, 推理过程千千万, 不强求. 甚至语言表达出来, 对于人类有启发.

# RL4LLM的挑战

众所周知, RL训练算法收敛慢, 不稳定, 更别说用在大语言模型上. 主要针对RL训练算法的优化.

PPO: Proximal Policy Optimization / 传统深度强化学习的算法

GRPO: Group Relative Policy Optimization / 去掉了评价模型

...



# TOREAD

https://huggingface.co/learn/deep-rl-course/